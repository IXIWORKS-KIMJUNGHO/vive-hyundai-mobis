"""
Stage 1: Bangs, Glasses, and Beard classification using OpenAI's CLIP model.
Uses the full face image for all detections in a single pass.
"""
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import torch
import numpy as np
from utils import get_config, get_logger
from pathlib import Path

logger = get_logger(__name__)

# ë¡œì»¬ ëª¨ë¸ ì €ì¥ ê²½ë¡œ (í”„ë¡œì íŠ¸ ë‚´ë¶€)
MODELS_DIR = Path(__file__).parent.parent / 'data' / 'models' / 'clip-vit-base-patch32'

class CLIPClassifier:
    def __init__(self, device):
        logger.info("Loading CLIP model (this might take a moment on first run)...")
        self.device = device
        self.config = get_config()
        model_name = "openai/clip-vit-base-patch32"

        # ë¡œì»¬ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë¡œì»¬ì—ì„œ ë¡œë“œ, ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ í›„ ì €ì¥
        if MODELS_DIR.exists() and (MODELS_DIR / "config.json").exists():
            logger.info(f"âœ… ë¡œì»¬ CLIP ëª¨ë¸ ë¡œë“œ: {MODELS_DIR}")
            self.model = CLIPModel.from_pretrained(MODELS_DIR).to(self.device)
            self.processor = CLIPProcessor.from_pretrained(MODELS_DIR)
        else:
            logger.info("ğŸ“¥ CLIP ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (ìµœì´ˆ 1íšŒë§Œ, ì•½ 600MB)")
            self.model = CLIPModel.from_pretrained(model_name).to(self.device)
            self.processor = CLIPProcessor.from_pretrained(model_name)

            # ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ì„ ë¡œì»¬ì— ì €ì¥
            MODELS_DIR.mkdir(parents=True, exist_ok=True)
            self.model.save_pretrained(MODELS_DIR)
            self.processor.save_pretrained(MODELS_DIR)
            logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {MODELS_DIR}")

        self.results = {}

    def classify(self, pil_img, face, landmarks):
        """
        Analyzes the full face image using CLIP for bangs, glasses, and beard detection.
        Returns 'Bangs' or 'No Bangs'.
        """
        logger.info("[Stage 1] Analyzing with CLIP (Full Face Image)...")
        
        # ì–¼êµ´ ì „ì²´ ì˜ì—­ ì‚¬ìš© - ëª¨ë“  íŒë‹¨ì— ë™ì¼í•œ ì´ë¯¸ì§€ ì‚¬ìš©
        face_image = pil_img.crop((face.left(), face.top(), face.right(), face.bottom()))

        # ëª¨ë“  íŠ¹ì§•ì„ í•œ ë²ˆì— íŒë‹¨í•  í”„ë¡¬í”„íŠ¸
        text_prompts = [
            # Bangs (ì•ë¨¸ë¦¬) - ë” êµ¬ì²´ì ìœ¼ë¡œ
            "a person with full bangs completely covering the forehead",
            "a person with no bangs and completely visible forehead",
            # Glasses (ì•ˆê²½)
            "a person wearing glasses",
            "a person not wearing glasses",
            # Beard (ìˆ˜ì—¼)
            "a person with beard or facial hair",
            "a person without beard, clean shaven"
        ]

        inputs = self.processor(text=text_prompts, images=face_image, return_tensors="pt", padding=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = self.model(**inputs)
        
        logits_per_image = outputs.logits_per_image
        probs = logits_per_image.softmax(dim=1).cpu().numpy()[0]

        self.results = {prompt: prob for prompt, prob in zip(text_prompts, probs)}

        logger.debug("--- CLIP Analysis Results (All Features) ---")
        for prompt, prob in self.results.items():
            logger.debug(f"  '{prompt}': {prob:.2%}")
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê²°ê³¼ ì¶”ì¶œ
        bangs_probs = [probs[0], probs[1]]  # ì•ë¨¸ë¦¬ ìˆìŒ, ì—†ìŒ
        glasses_probs = [probs[2], probs[3]]  # ì•ˆê²½ ìˆìŒ, ì—†ìŒ
        beard_probs = [probs[4], probs[5]]  # ìˆ˜ì—¼ ìˆìŒ, ì—†ìŒ
        
        # íŒë‹¨
        bangs_result = "Bangs" if bangs_probs[0] > bangs_probs[1] else "No Bangs"
        glasses_result = "Wearing Glasses" if glasses_probs[0] > glasses_probs[1] else "No Glasses"
        beard_result = "With Beard" if beard_probs[0] > beard_probs[1] else "No Beard"
        
        logger.info("=" * 60)
        logger.info("FINAL CLIP RESULTS:")
        logger.info(f"  Bangs: {bangs_result} (confidence: {max(bangs_probs):.2%})")
        logger.info(f"  Glasses: {glasses_result} (confidence: {max(glasses_probs):.2%})")
        logger.info(f"  Beard: {beard_result} (confidence: {max(beard_probs):.2%})")
        logger.info("=" * 60)

        if bangs_result == "Bangs":
            logger.info("CLIP Conclusion: Bangs DETECTED.")
        else:
            logger.info("CLIP Conclusion: No Bangs detected. Proceeding to Stage 2.")
        
        return bangs_result

    def get_additional_results(self):
        """ì•ˆê²½ê³¼ ìˆ˜ì—¼ íŒë‹¨ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not self.results:
            return {
                'glasses': 'Unknown',
                'glasses_confidence': 0.0,
                'beard': 'Unknown',
                'beard_confidence': 0.0
            }
        
        # í”„ë¡¬í”„íŠ¸ì—ì„œ í™•ë¥ ê°’ ì¶”ì¶œ
        prompts_list = list(self.results.keys())
        probs_list = list(self.results.values())
        
        glasses_probs = [probs_list[2], probs_list[3]]  # ì•ˆê²½ ìˆìŒ, ì—†ìŒ
        beard_probs = [probs_list[4], probs_list[5]]  # ìˆ˜ì—¼ ìˆìŒ, ì—†ìŒ
        
        glasses_status = "Wearing Glasses" if glasses_probs[0] > glasses_probs[1] else "No Glasses"
        beard_status = "With Beard" if beard_probs[0] > beard_probs[1] else "No Beard"
        
        return {
            'glasses': glasses_status,
            'glasses_confidence': max(glasses_probs),
            'beard': beard_status,
            'beard_confidence': max(beard_probs)
        }
    
    def classify_accessories_only(self, pil_img, face, landmarks):
        """
        ì•ˆê²½, ìˆ˜ì—¼, ì„±ë³„ íŒë‹¨ (ì•ë¨¸ë¦¬ëŠ” BiSeNetì´ ë‹´ë‹¹)
        """
        logger.info("[CLIP] Analyzing accessories and gender (glasses, beard, gender)...")

        face_image = pil_img.crop((face.left(), face.top(), face.right(), face.bottom()))

        text_prompts = [
            "a person wearing glasses",
            "a person not wearing glasses",
            "a person with beard or facial hair",
            "a person without beard, clean shaven",
            "a male person",
            "a female person"
        ]

        inputs = self.processor(text=text_prompts, images=face_image, return_tensors="pt", padding=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = self.model(**inputs)

        logits_per_image = outputs.logits_per_image
        probs = logits_per_image.softmax(dim=1).cpu().numpy()[0]

        # Threshold ê¸°ë°˜ íŒì • (confidenceê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì‹ ë¢°í•˜ì§€ ì•ŠìŒ)
        glasses_confidence = max(probs[0], probs[1])
        glasses_threshold = self.config.clip.glasses_confidence_threshold
        if glasses_confidence < glasses_threshold:
            glasses_result = "No Glasses"  # ê¸°ë³¸ê°’
        else:
            glasses_result = "Wearing Glasses" if probs[0] > probs[1] else "No Glasses"

        # ìˆ˜ì—¼ íŒì •: ìµœì†Œ threshold ì´ìƒ confidence í•„ìš”
        beard_confidence = max(probs[2], probs[3])
        beard_threshold = self.config.clip.beard_confidence_threshold
        if beard_confidence < beard_threshold:
            beard_result = "No Beard"  # í™•ì‹ ì´ ë‚®ìœ¼ë©´ ê¸°ë³¸ê°’
        else:
            beard_result = "With Beard" if probs[2] > probs[3] else "No Beard"

        # ì„±ë³„ íŒì •: ìƒëŒ€ ë¹„êµ (NIR ì´ë¯¸ì§€ì—ì„œëŠ” confidenceê°€ ë‚®ì„ ìˆ˜ ìˆìŒ)
        gender_result = "Male" if probs[4] > probs[5] else "Female"

        logger.info(f"  Glasses: {glasses_result} ({max(probs[0], probs[1]):.2%})")
        logger.info(f"  Beard: {beard_result} ({max(probs[2], probs[3]):.2%})")
        logger.info(f"  Gender: {gender_result} ({max(probs[4], probs[5]):.2%})")

        return {
            'glasses': glasses_result,
            'glasses_confidence': max(probs[0], probs[1]),
            'beard': beard_result,
            'beard_confidence': max(probs[2], probs[3]),
            'gender': gender_result,
            'gender_confidence': max(probs[4], probs[5])
        }